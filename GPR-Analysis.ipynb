{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea60d3b-f67a-46cf-b8c5-05ae26da8bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def mRmR(df, feature_columns, target_column, shuffletime=5, maxpredictors=5):\n",
    "    selected_features = []\n",
    "    features_left = feature_columns.copy()\n",
    "    ForPlot=False\n",
    "    FinalBestRMSE=float('inf')\n",
    "    for i in range(1, maxpredictors+1):\n",
    "        r2_scores = []\n",
    "        mean_r2_scores = []\n",
    "        std_r2_scores = []\n",
    "\n",
    "        rmse_scores = []\n",
    "        mean_rmse_scores = []\n",
    "        std_rmse_scores = []\n",
    "        rmse_scores_train = []\n",
    "\n",
    "        # For the first iteration, evaluate each feature individually\n",
    "        if i == 1:\n",
    "            features_to_evaluate = features_left\n",
    "        # For subsequent iterations, combine the selected features with each remaining feature\n",
    "        else:\n",
    "            features_to_evaluate = [selected_features + [feature] for feature in features_left]\n",
    "        if len(features_to_evaluate)!=0:\n",
    "            for features in features_to_evaluate:\n",
    "                print(\"Currently selected features:\",features)\n",
    "                # If features is a list (combined features), join them for GPR_FOR_Shuffletimes\n",
    "                if isinstance(features, list):\n",
    "                    current_features = features\n",
    "                else:\n",
    "                    current_features = [features]\n",
    "    \n",
    "                # Call GPR_FOR_Shuffletimes with the current feature(s)\n",
    "                best_rmse, best_r2_for_best_rmse, mean_rmse, std_rmse, mean_r2, std_r2,best_rmse_train = GPR_FOR_Shuffletimes(\n",
    "                    df, current_features, target_column,ForPlot, test_size=0.2,\n",
    "                    fixed_length_scale=False, kernel_length_scale=1,\n",
    "                    shuffle_times=shuffletime\n",
    "                )\n",
    "    \n",
    "                # Store scores\n",
    "                r2_scores.append(best_r2_for_best_rmse)\n",
    "                mean_r2_scores.append(mean_r2)\n",
    "                std_r2_scores.append(std_r2)\n",
    "    \n",
    "                rmse_scores.append(best_rmse)\n",
    "                mean_rmse_scores.append(mean_rmse)\n",
    "                std_rmse_scores.append(std_rmse)\n",
    "                rmse_scores_train.append(best_rmse_train)\n",
    "                \n",
    "                print(f\"BestR2Value: {best_r2_for_best_rmse:.3f}, AverageR2Value: {mean_r2:.3f}, StdR2Value: {std_r2:.3f}\")\n",
    "                print(f\"Best_RMSE_test: {best_rmse:.4f}, Best_RMSE_train: {best_rmse_train:.4f}, AverageRMSEValue: {mean_rmse:.4f}, StdRMSEValue: {std_rmse:.5f}\")\n",
    "                print(\"----------------------------------------------\")\n",
    "        # Find the best feature (or combination) based on RMSE\n",
    "        best_idx = np.argmin(rmse_scores)\n",
    "\n",
    "        if FinalBestRMSE*0.99<rmse_scores[best_idx]: #The improvements has to be higher than 1 % else ignored\n",
    "            print(\"No improvement detected - revert to previous best combination\")\n",
    "            print(\"----------------------------------------------\")\n",
    "            break\n",
    "        else:\n",
    "            best_feature_or_combination = features_to_evaluate[best_idx]\n",
    "\n",
    "            FinalBestR2=r2_scores[best_idx]\n",
    "            FinalMeanR2=mean_r2_scores[best_idx]        \n",
    "            FinalStdR2=std_r2_scores[best_idx]\n",
    "            \n",
    "            FinalBestRMSE=rmse_scores[best_idx]\n",
    "            FinalMeanRMSE=mean_rmse_scores[best_idx]        \n",
    "            FinalStdRMSE=std_rmse_scores[best_idx]\n",
    "            FinalBestRMSETrain=rmse_scores_train[best_idx]\n",
    "\n",
    "    \n",
    "            # Update selected_features and features_left\n",
    "            if isinstance(best_feature_or_combination, list):\n",
    "                new_feature = [f for f in best_feature_or_combination if f not in selected_features][0]\n",
    "                selected_features.append(new_feature)\n",
    "                features_left.remove(new_feature)\n",
    "            else:\n",
    "                selected_features.append(best_feature_or_combination)\n",
    "                features_left.remove(best_feature_or_combination)\n",
    "\n",
    "        # Print or store the best feature/combination for this iteration\n",
    "\n",
    "        print(f\"Iteration {i}: Selected {best_feature_or_combination} with RMSE = {rmse_scores[best_idx]:.4f}\")\n",
    "        print(\"----------------------------------------------\")\n",
    "\n",
    "    ForPlot=True\n",
    "\n",
    "    # Call GPR_FOR_Shuffletimes with the current feature(s)\n",
    "    best_rmse, best_r2_for_best_rmse, mean_rmse, std_rmse, mean_r2, std_r2,best_rmse_train = GPR_FOR_Shuffletimes(\n",
    "                df, best_feature_or_combination, target_column,ForPlot, test_size=0.2,\n",
    "                fixed_length_scale=False, kernel_length_scale=1,\n",
    "                shuffle_times=20\n",
    "            )    \n",
    "    \n",
    "    return selected_features,[FinalBestR2,FinalMeanR2,FinalStdR2],[FinalBestRMSE,FinalMeanRMSE,FinalStdRMSE,FinalBestRMSETrain]\n",
    "\n",
    "        \n",
    "def GPR_FOR_Shuffletimes(df, feature_columns, target_column,ForPlot, test_size=0.2, fixed_length_scale=False, kernel_length_scale=1, shuffle_times=5):\n",
    "\n",
    "    # Extract features and target from DataFrame\n",
    "    X_final = df[feature_columns].values.astype(float)\n",
    "    y = df[target_column].values.astype(float)\n",
    "    \n",
    "    # Ensure X_final is 2D\n",
    "    if X_final.ndim == 1:\n",
    "        X_final = X_final.reshape(-1, 1)\n",
    "\n",
    "    # Kernel Definition\n",
    "    optimizer_setting = 'fmin_l_bfgs_b'\n",
    "    noise_level = 1e-5  # Initial guess for noise variance (adjust as needed)\n",
    "\n",
    "    if fixed_length_scale:\n",
    "        kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=kernel_length_scale, length_scale_bounds='fixed') + WhiteKernel(noise_level=noise_level, noise_level_bounds=(1e-10, 1e+1))\n",
    "        optimizer_setting = None\n",
    "    else:\n",
    "        kernel = C(1.0, (10, 1e4)) * RBF(length_scale=kernel_length_scale, length_scale_bounds=(10, 1e3)) + WhiteKernel(noise_level=noise_level, noise_level_bounds=(1e-10, 1e+1))\n",
    "    \n",
    "\n",
    "    gpr = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        n_restarts_optimizer=10,\n",
    "        optimizer=optimizer_setting,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Standardize X_final\n",
    "    X_final, X_m, X_s = standadize(X_final)\n",
    "\n",
    "    # Initialize lists to store scores\n",
    "    r2_scores = []\n",
    "    rmse_scores = []\n",
    "    rmse_scores_train = []\n",
    "\n",
    "    # Initialize variables to track the best RMSE and its corresponding R²\n",
    "    best_rmse = float('inf')\n",
    "    best_r2_for_best_rmse = None\n",
    "\n",
    "    # Loop over shuffles\n",
    "    for shuffle in range(shuffle_times):\n",
    "        current_seed = 42 + shuffle\n",
    "\n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(\n",
    "            X_final, y, np.arange(len(df)), test_size=test_size, random_state=current_seed\n",
    "        )\n",
    "        # Train and Predict\n",
    "        gpr.fit(X_train, y_train)\n",
    "        y_pred_train = gpr.predict(X_train)\n",
    "        y_pred = gpr.predict(X_test)\n",
    "\n",
    "        # Calculate Metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        rmsetrain = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Store scores\n",
    "        r2_scores.append(r2)\n",
    "        rmse_scores.append(rmse)\n",
    "        rmse_scores_train.append(rmsetrain)\n",
    "        \n",
    "        # Update best RMSE and corresponding results\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_rmse_train = rmsetrain\n",
    "            best_r2_for_best_rmse = r2\n",
    "            best_shuffle_seed = current_seed\n",
    "            best_y_train, best_y_pred_train, best_y_test, best_y_pred = y_train, y_pred_train, y_test, y_pred\n",
    "\n",
    "    # After all shuffles, plot the best result\n",
    "    if ForPlot and best_y_train is not None:\n",
    "        plot_train_test_scatter(best_y_train, best_y_pred_train, best_y_test, best_y_pred,target_column)\n",
    "        print(f\"Plotted results for shuffle with best RMSE (seed={best_shuffle_seed}, RMSE_test={best_rmse:.4f}, R²={best_r2_for_best_rmse:.4f})\")\n",
    "    # Calculate cross-validation scores (mean and std)\n",
    "    mean_rmse = np.mean(rmse_scores)\n",
    "    std_rmse = np.std(rmse_scores)\n",
    "    mean_r2 = np.mean(r2_scores)\n",
    "    std_r2 = np.std(r2_scores)\n",
    "\n",
    "    return best_rmse, best_r2_for_best_rmse, mean_rmse, std_rmse, mean_r2, std_r2,best_rmse_train\n",
    "\n",
    "def standadize(X):\n",
    "    X = np.array(X)\n",
    "    X_m = np.mean(X, axis=0)\n",
    "    X_s = np.std(X, axis=0)\n",
    "    X -= X_m\n",
    "    X /= X_s\n",
    "    return X, X_m, X_s  # Return mean and std for unstandardization\n",
    "\n",
    "def unstandardize(X, X_m, X_s):\n",
    "    X = np.array(X)\n",
    "    X *= X_s\n",
    "    X += X_m\n",
    "    return X\n",
    "\n",
    "def plot_train_test_scatter(X_train, y_train, X_test, y_pred,target_column):\n",
    "    \"\"\"\n",
    "    Plots training data and test predictions as a scatter plot.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: array-like, training feature values\n",
    "    - y_train: array-like, training target values\n",
    "    - X_test: array-like, test feature values\n",
    "    - y_pred: array-like, predicted target values for test data\n",
    "    - title: str, title of the plot (default: 'Scatter Plot: Training vs. Test Predictions')\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_train, y_train, color='blue', label='Training Data')\n",
    "    plt.scatter(X_test, y_pred, color='red', label='Test Predictions')\n",
    "\n",
    "    plt.xlabel(\"Experimental \" + target_column)\n",
    "    plt.ylabel(\"Predicted \" + target_column)\n",
    "    #plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "##############################################################\n",
    "############## targets examples\n",
    "#  IV :  \"PCE_IV\",\"VOC_IV\",\"JSC_IV\",\"FF_IV\"\n",
    "\n",
    "#  Spectral features: 'features_A.1.c_UV','features_A.1.w_UV','features_A.1a.a_UV','features_A.1a.c_UV','features_ADRatio','features_A_UV','features_Atot_UV',\n",
    "# 'features_D.1.c_UV', 'features_D.1.w_UV', 'features_D.1a.a_UV', 'features_D.1a.c_UV', 'features_D_UV', 'features_Dtot_UV', 'features_XA_UV', 'features_XD_UV',\n",
    "# 'features_XamA_UV', 'features_XamD_UV', 'features_atot_UV'\n",
    "\n",
    "target_column = \"VOC_IV\"\n",
    "\n",
    "############## features examples\n",
    "# There are two feature sets. They can be selected with\n",
    "# plan parameters:Features = \"plan_\"\n",
    "#Features = \"plan_\"\n",
    "\n",
    "# spectral features: Features = \"features_\"\n",
    "Features = \"features_\"\n",
    "\n",
    "# Extracts all columns beginning with string defined in \"Features\"\n",
    "feature_columns = [col for col in df.columns if col.startswith(Features)]\n",
    "\n",
    "############## Read in data\n",
    "df = pd.read_csv(\"Optimization data set.csv\")\n",
    "\n",
    "\n",
    "\n",
    "############## Set shuffle times and max predictors\n",
    "\n",
    "selected,ListResultR2,ListResultRMSE = mRmR(df, feature_columns, target_column,shuffletime=20, maxpredictors=5)\n",
    "\n",
    "##############################################################\n",
    "\n",
    "# Output\n",
    "\n",
    "print(\"Selected features:\", selected)\n",
    "print(f\"BestR2Value: {ListResultR2[0]:.3f}, AverageR2Value: {ListResultR2[1]:.3f}, StdR2Value: {ListResultR2[2]:.4f}\")\n",
    "print(f\"Best_RMSE_test: {ListResultRMSE[0]:.4f}, Best_RMSE_train: {ListResultRMSE[3]:.4f}, AverageRMSEValue: {ListResultRMSE[1]:.4f}, StdRMSEValue: {ListResultRMSE[2]:.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jonas",
   "language": "python",
   "name": "jonas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
